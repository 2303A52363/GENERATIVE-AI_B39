{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuD3b13jSJa/IItUOM2HFl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A52363/GENERATIVE-AI_B39/blob/main/ASSIGNMENT_6(GEN_AI).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ASSIGNMENT - 6\n",
        "\n",
        "HALLTICKET NO : 2303A52363\n",
        "\n",
        "BATCH : 39"
      ],
      "metadata": {
        "id": "l8f6i9LLtB0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"/content/Housing.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Identify numerical and categorical columns\n",
        "num_features = [\"area\", \"bedrooms\", \"bathrooms\", \"stories\", \"parking\"]\n",
        "cat_features = [\"mainroad\", \"guestroom\", \"basement\", \"hotwaterheating\", \"airconditioning\", \"prefarea\", \"furnishingstatus\"]\n",
        "\n",
        "# Define preprocessors\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), num_features),\n",
        "        (\"cat\", OneHotEncoder(drop=\"first\"), cat_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Prepare data\n",
        "X = df.drop(columns=[\"price\"])\n",
        "y = df[\"price\"]\n",
        "X_transformed = preprocessor.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the Random Forest Regressor model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = rf_model.predict(X_train)\n",
        "y_test_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate error metrics\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "\n",
        "# Save the trained model\n",
        "joblib.dump(rf_model, \"housing_price_model.pkl\")\n",
        "\n",
        "# Print model performance\n",
        "print(f\"Training MSE: {train_mse:.2f}\")\n",
        "print(f\"Training MAE: {train_mae:.2f}\")\n",
        "print(f\"Testing MSE: {test_mse:.2f}\")\n",
        "print(f\"Testing MAE: {test_mae:.2f}\")\n",
        "\n",
        "print(\"Model saved as 'housing_price_model.pkl'.\")\n",
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "# Load the saved model\n",
        "rf_model = joblib.load(\"housing_price_model.pkl\")\n",
        "\n",
        "# Predict on a new sample (example input with 13 features)\n",
        "new_sample = np.array([X_test[0]])  # Use any sample from X_test\n",
        "predicted_price = rf_model.predict(new_sample)\n",
        "\n",
        "print(\"Predicted Housing Price:\", predicted_price[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDdk5B00nQPl",
        "outputId": "a31533cc-4db1-4d12-fbe8-931936a5a2ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training MSE: 153669205868.61\n",
            "Training MAE: 277348.26\n",
            "Testing MSE: 1959406221695.99\n",
            "Testing MAE: 1017470.62\n",
            "Model saved as 'housing_price_model.pkl'.\n",
            "Predicted Housing Price: 5344780.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import h5py\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "url = \"/content/Housing.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Data Preprocessing\n",
        "# Separate features (X) and target (y) columns\n",
        "X = df.drop(columns='price')  # Replace 'Price' with the actual column name for housing price\n",
        "y = df['price']  # Replace 'Price' with the actual column name for housing price\n",
        "\n",
        "# Identify numerical and categorical features\n",
        "numerical_features = X.select_dtypes(include=['number']).columns\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Create a ColumnTransformer to apply different preprocessing to numerical and categorical features\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), categorical_features),  # Use OneHotEncoder for categorical features\n",
        "    ])\n",
        "\n",
        "# Apply the preprocessing to the features\n",
        "X_scaled = preprocessor.fit_transform(X)\n",
        "\n",
        "# Step 3: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "# Step 4: Define the model architecture\n",
        "model = Sequential()\n",
        "\n",
        "# Hidden Layer 1\n",
        "model.add(Dense(15, input_dim=X_train.shape[1], activation='relu'))\n",
        "\n",
        "# Hidden Layer 2\n",
        "model.add(Dense(20, activation='relu'))\n",
        "\n",
        "# Hidden Layer 3\n",
        "model.add(Dense(25, activation='relu'))\n",
        "\n",
        "# Hidden Layer 4\n",
        "model.add(Dense(20, activation='relu'))\n",
        "\n",
        "# Hidden Layer 5\n",
        "model.add(Dense(15, activation='relu'))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(1))  # A single output for regression (housing price)\n",
        "\n",
        "# Step 5: Compile the model\n",
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['mae'])  # Mean Absolute Error as an evaluation metric\n",
        "\n",
        "# Step 6: Train the model\n",
        "history = model.fit(X_train, y_train, epochs=150, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Step 7: Calculate training and testing error metrics\n",
        "# Calculate the training MAE (Mean Absolute Error)\n",
        "train_mae = history.history['mae'][-1]\n",
        "print(f\"Training Mean Absolute Error: {train_mae}\")\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
        "print(f\"Testing Mean Absolute Error: {test_mae}\")\n",
        "\n",
        "# Step 8: Save the trained model to .h5 file\n",
        "model.save('housing_price_model.h5')\n",
        "print(\"Model saved as 'housing_price_model.h5'\")\n",
        "\n",
        "# Step 9: Load the saved model (for deployment or testing)\n",
        "from keras.models import load_model\n",
        "\n",
        "# Load the model\n",
        "loaded_model = load_model('housing_price_model.h5')\n",
        "\n",
        "# Step 10: Use the loaded model to make predictions on test data\n",
        "predictions = loaded_model.predict(X_test)\n",
        "\n",
        "# Step 11: Calculate the MAE for predictions\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "print(f\"Prediction Mean Absolute Error: {mae}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U804-P41XdCI",
        "outputId": "03c29c4a-104f-48d5-cce8-00d908e4bda5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - loss: 25298861031424.0000 - mae: 4724260.5000 - val_loss: 30129988304896.0000 - val_mae: 5007536.0000\n",
            "Epoch 2/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 23608386650112.0000 - mae: 4537649.5000 - val_loss: 30129984110592.0000 - val_mae: 5007535.5000\n",
            "Epoch 3/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 24668742352896.0000 - mae: 4667932.0000 - val_loss: 30129971527680.0000 - val_mae: 5007534.5000\n",
            "Epoch 4/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 25560549949440.0000 - mae: 4732598.0000 - val_loss: 30129944264704.0000 - val_mae: 5007531.5000\n",
            "Epoch 5/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 24347349614592.0000 - mae: 4664358.5000 - val_loss: 30129875058688.0000 - val_mae: 5007524.5000\n",
            "Epoch 6/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 24221518397440.0000 - mae: 4617609.5000 - val_loss: 30129700995072.0000 - val_mae: 5007507.0000\n",
            "Epoch 7/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 25675121557504.0000 - mae: 4767578.0000 - val_loss: 30129306730496.0000 - val_mae: 5007468.0000\n",
            "Epoch 8/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 24611492200448.0000 - mae: 4648271.0000 - val_loss: 30128432218112.0000 - val_mae: 5007381.0000\n",
            "Epoch 9/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 24250303905792.0000 - mae: 4624514.5000 - val_loss: 30126620278784.0000 - val_mae: 5007202.5000\n",
            "Epoch 10/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 25657601949696.0000 - mae: 4725282.5000 - val_loss: 30123078189056.0000 - val_mae: 5006855.0000\n",
            "Epoch 11/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25919687229440.0000 - mae: 4736694.0000 - val_loss: 30116507811840.0000 - val_mae: 5006215.5000\n",
            "Epoch 12/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25950681038848.0000 - mae: 4757914.0000 - val_loss: 30104384176128.0000 - val_mae: 5005042.0000\n",
            "Epoch 13/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 24598896705536.0000 - mae: 4662212.5000 - val_loss: 30083502833664.0000 - val_mae: 5003024.5000\n",
            "Epoch 14/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 24634313408512.0000 - mae: 4668607.5000 - val_loss: 30048809648128.0000 - val_mae: 4999685.5000\n",
            "Epoch 15/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25092339793920.0000 - mae: 4689121.5000 - val_loss: 29993694396416.0000 - val_mae: 4994392.5000\n",
            "Epoch 16/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 24027083046912.0000 - mae: 4602529.5000 - val_loss: 29908992524288.0000 - val_mae: 4986284.0000\n",
            "Epoch 17/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25687410868224.0000 - mae: 4739996.5000 - val_loss: 29781728952320.0000 - val_mae: 4974128.5000\n",
            "Epoch 18/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25559956455424.0000 - mae: 4718698.0000 - val_loss: 29599293505536.0000 - val_mae: 4956653.0000\n",
            "Epoch 19/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 24436042366976.0000 - mae: 4663121.5000 - val_loss: 29339533967360.0000 - val_mae: 4931751.0000\n",
            "Epoch 20/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 23467548213248.0000 - mae: 4561878.0000 - val_loss: 28985914294272.0000 - val_mae: 4897641.5000\n",
            "Epoch 21/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 23555620208640.0000 - mae: 4543906.5000 - val_loss: 28503690969088.0000 - val_mae: 4850923.5000\n",
            "Epoch 22/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 24248299028480.0000 - mae: 4588055.0000 - val_loss: 27865194168320.0000 - val_mae: 4788449.0000\n",
            "Epoch 23/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 22495916720128.0000 - mae: 4434550.5000 - val_loss: 27048735145984.0000 - val_mae: 4707365.0000\n",
            "Epoch 24/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 22219398840320.0000 - mae: 4409403.0000 - val_loss: 26012073066496.0000 - val_mae: 4602510.0000\n",
            "Epoch 25/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 21704864694272.0000 - mae: 4331901.5000 - val_loss: 24724795031552.0000 - val_mae: 4469295.5000\n",
            "Epoch 26/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 19909039882240.0000 - mae: 4120222.0000 - val_loss: 23195505655808.0000 - val_mae: 4305593.0000\n",
            "Epoch 27/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 17939908526080.0000 - mae: 3914367.7500 - val_loss: 21372008923136.0000 - val_mae: 4102741.2500\n",
            "Epoch 28/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 17800137539584.0000 - mae: 3850840.7500 - val_loss: 19278426275840.0000 - val_mae: 3857316.5000\n",
            "Epoch 29/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14483447611392.0000 - mae: 3464171.5000 - val_loss: 16973769474048.0000 - val_mae: 3568663.7500\n",
            "Epoch 30/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13380832198656.0000 - mae: 3242698.2500 - val_loss: 14513395990528.0000 - val_mae: 3233807.2500\n",
            "Epoch 31/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10508616859648.0000 - mae: 2875038.2500 - val_loss: 12014532427776.0000 - val_mae: 2855880.7500\n",
            "Epoch 32/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8982460628992.0000 - mae: 2570266.0000 - val_loss: 9611854741504.0000 - val_mae: 2457903.2500\n",
            "Epoch 33/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5740052873216.0000 - mae: 1988236.8750 - val_loss: 7486549524480.0000 - val_mae: 2086008.6250\n",
            "Epoch 34/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4871506886656.0000 - mae: 1765645.6250 - val_loss: 5682884509696.0000 - val_mae: 1744414.3750\n",
            "Epoch 35/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3256903008256.0000 - mae: 1341442.5000 - val_loss: 4383665291264.0000 - val_mae: 1515466.1250\n",
            "Epoch 36/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2408799010816.0000 - mae: 1120243.5000 - val_loss: 3525092311040.0000 - val_mae: 1368789.2500\n",
            "Epoch 37/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1747264864256.0000 - mae: 966466.5000 - val_loss: 3056556834816.0000 - val_mae: 1293293.1250\n",
            "Epoch 38/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1836280315904.0000 - mae: 963512.1250 - val_loss: 2777713999872.0000 - val_mae: 1248866.5000\n",
            "Epoch 39/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1490285756416.0000 - mae: 894277.4375 - val_loss: 2647855726592.0000 - val_mae: 1229245.2500\n",
            "Epoch 40/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1695255756800.0000 - mae: 928356.8750 - val_loss: 2571791237120.0000 - val_mae: 1215405.1250\n",
            "Epoch 41/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1521050451968.0000 - mae: 925683.4375 - val_loss: 2519271473152.0000 - val_mae: 1202951.5000\n",
            "Epoch 42/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1204774764544.0000 - mae: 822825.6875 - val_loss: 2481708335104.0000 - val_mae: 1192188.8750\n",
            "Epoch 43/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1275728232448.0000 - mae: 826257.8750 - val_loss: 2437362221056.0000 - val_mae: 1180923.7500\n",
            "Epoch 44/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1347561979904.0000 - mae: 871815.3750 - val_loss: 2401671053312.0000 - val_mae: 1170213.5000\n",
            "Epoch 45/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1451606409216.0000 - mae: 893413.6875 - val_loss: 2372318003200.0000 - val_mae: 1160977.6250\n",
            "Epoch 46/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1369600294912.0000 - mae: 860353.0000 - val_loss: 2343456735232.0000 - val_mae: 1152208.8750\n",
            "Epoch 47/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1383849000960.0000 - mae: 883024.8125 - val_loss: 2323886899200.0000 - val_mae: 1145104.5000\n",
            "Epoch 48/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1427596640256.0000 - mae: 878298.0000 - val_loss: 2295802888192.0000 - val_mae: 1137728.0000\n",
            "Epoch 49/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1389880279040.0000 - mae: 850880.7500 - val_loss: 2270649647104.0000 - val_mae: 1130712.2500\n",
            "Epoch 50/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1095328858112.0000 - mae: 798942.6250 - val_loss: 2252079628288.0000 - val_mae: 1124931.5000\n",
            "Epoch 51/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1289804709888.0000 - mae: 837471.5000 - val_loss: 2225875189760.0000 - val_mae: 1118963.6250\n",
            "Epoch 52/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1135832072192.0000 - mae: 802544.3125 - val_loss: 2204122480640.0000 - val_mae: 1113226.1250\n",
            "Epoch 53/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1177353453568.0000 - mae: 806647.5625 - val_loss: 2188233932800.0000 - val_mae: 1108137.7500\n",
            "Epoch 54/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1261599195136.0000 - mae: 834371.6250 - val_loss: 2172405678080.0000 - val_mae: 1103112.2500\n",
            "Epoch 55/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1150401511424.0000 - mae: 795750.3750 - val_loss: 2153724903424.0000 - val_mae: 1098108.5000\n",
            "Epoch 56/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1584646062080.0000 - mae: 909150.6875 - val_loss: 2127411675136.0000 - val_mae: 1092283.5000\n",
            "Epoch 57/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1237164097536.0000 - mae: 824601.0000 - val_loss: 2116617633792.0000 - val_mae: 1088607.6250\n",
            "Epoch 58/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1312818200576.0000 - mae: 833879.8750 - val_loss: 2106423902208.0000 - val_mae: 1084531.5000\n",
            "Epoch 59/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1181800988672.0000 - mae: 793814.2500 - val_loss: 2098852921344.0000 - val_mae: 1081195.2500\n",
            "Epoch 60/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1169937924096.0000 - mae: 780489.4375 - val_loss: 2081876082688.0000 - val_mae: 1077254.5000\n",
            "Epoch 61/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1190429196288.0000 - mae: 791833.9375 - val_loss: 2061982236672.0000 - val_mae: 1073031.1250\n",
            "Epoch 62/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1230100889600.0000 - mae: 796143.3750 - val_loss: 2051569090560.0000 - val_mae: 1069837.1250\n",
            "Epoch 63/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1235244417024.0000 - mae: 804847.8125 - val_loss: 2039076880384.0000 - val_mae: 1066240.1250\n",
            "Epoch 64/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1162003218432.0000 - mae: 763545.1875 - val_loss: 2022709657600.0000 - val_mae: 1062431.2500\n",
            "Epoch 65/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1038202503168.0000 - mae: 750634.0000 - val_loss: 2028065390592.0000 - val_mae: 1061424.1250\n",
            "Epoch 66/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 945066278912.0000 - mae: 724366.7500 - val_loss: 2019289858048.0000 - val_mae: 1058521.8750\n",
            "Epoch 67/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1168116678656.0000 - mae: 801674.8750 - val_loss: 2002279858176.0000 - val_mae: 1054775.8750\n",
            "Epoch 68/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1244541485056.0000 - mae: 779254.5000 - val_loss: 1986783870976.0000 - val_mae: 1051052.3750\n",
            "Epoch 69/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1209796526080.0000 - mae: 802548.5625 - val_loss: 1987584196608.0000 - val_mae: 1049373.1250\n",
            "Epoch 70/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1193658548224.0000 - mae: 789419.6875 - val_loss: 1974141976576.0000 - val_mae: 1046096.2500\n",
            "Epoch 71/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1115068432384.0000 - mae: 777532.5625 - val_loss: 1975756390400.0000 - val_mae: 1044623.1250\n",
            "Epoch 72/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1012445151232.0000 - mae: 739016.8125 - val_loss: 1968811540480.0000 - val_mae: 1041964.3125\n",
            "Epoch 73/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1131491098624.0000 - mae: 781321.0625 - val_loss: 1946702839808.0000 - val_mae: 1037731.0625\n",
            "Epoch 74/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1121058291712.0000 - mae: 768224.8750 - val_loss: 1938222481408.0000 - val_mae: 1035226.4375\n",
            "Epoch 75/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1212041265152.0000 - mae: 795077.5000 - val_loss: 1933423411200.0000 - val_mae: 1032823.7500\n",
            "Epoch 76/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1082402078720.0000 - mae: 747057.5625 - val_loss: 1930723196928.0000 - val_mae: 1031144.2500\n",
            "Epoch 77/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1007951806464.0000 - mae: 726362.8125 - val_loss: 1931221532672.0000 - val_mae: 1029810.0625\n",
            "Epoch 78/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 952094425088.0000 - mae: 713932.0000 - val_loss: 1921354301440.0000 - val_mae: 1027066.9375\n",
            "Epoch 79/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1100120981504.0000 - mae: 763358.8750 - val_loss: 1912779964416.0000 - val_mae: 1024617.3125\n",
            "Epoch 80/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1181629153280.0000 - mae: 790601.2500 - val_loss: 1903735078912.0000 - val_mae: 1021993.5625\n",
            "Epoch 81/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1029503254528.0000 - mae: 749402.6875 - val_loss: 1904706191360.0000 - val_mae: 1020804.6250\n",
            "Epoch 82/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1060324114432.0000 - mae: 720175.6250 - val_loss: 1895618445312.0000 - val_mae: 1018491.1250\n",
            "Epoch 83/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1220602363904.0000 - mae: 788371.4375 - val_loss: 1888515260416.0000 - val_mae: 1016242.8750\n",
            "Epoch 84/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1085007527936.0000 - mae: 763667.0625 - val_loss: 1895016431616.0000 - val_mae: 1015433.0000\n",
            "Epoch 85/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1212875669504.0000 - mae: 780692.2500 - val_loss: 1893119295488.0000 - val_mae: 1014020.3750\n",
            "Epoch 86/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 997715738624.0000 - mae: 729730.6875 - val_loss: 1895353942016.0000 - val_mae: 1013021.4375\n",
            "Epoch 87/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1013698920448.0000 - mae: 729500.1875 - val_loss: 1876880654336.0000 - val_mae: 1010550.0000\n",
            "Epoch 88/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 888364859392.0000 - mae: 692222.3750 - val_loss: 1870949253120.0000 - val_mae: 1008825.8125\n",
            "Epoch 89/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1099436261376.0000 - mae: 763634.0625 - val_loss: 1864218836992.0000 - val_mae: 1008499.9375\n",
            "Epoch 90/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 976670097408.0000 - mae: 730951.8750 - val_loss: 1863403831296.0000 - val_mae: 1006736.0000\n",
            "Epoch 91/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 895288016896.0000 - mae: 695770.5000 - val_loss: 1869710884864.0000 - val_mae: 1005726.3125\n",
            "Epoch 92/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1120630341632.0000 - mae: 771289.5625 - val_loss: 1854972100608.0000 - val_mae: 1004761.5625\n",
            "Epoch 93/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1003044077568.0000 - mae: 743824.6875 - val_loss: 1853672128512.0000 - val_mae: 1004074.2500\n",
            "Epoch 94/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1197366181888.0000 - mae: 780046.5000 - val_loss: 1848675270656.0000 - val_mae: 1003724.5000\n",
            "Epoch 95/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1016610947072.0000 - mae: 734011.8125 - val_loss: 1850337132544.0000 - val_mae: 1002595.7500\n",
            "Epoch 96/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1076093911040.0000 - mae: 741177.1875 - val_loss: 1847736401920.0000 - val_mae: 1001580.1875\n",
            "Epoch 97/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1105551687680.0000 - mae: 743079.4375 - val_loss: 1863778959360.0000 - val_mae: 998842.1875\n",
            "Epoch 98/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1206897475584.0000 - mae: 793604.8750 - val_loss: 1839546368000.0000 - val_mae: 1001404.6250\n",
            "Epoch 99/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1039016001536.0000 - mae: 750457.0000 - val_loss: 1836276252672.0000 - val_mae: 1000859.3125\n",
            "Epoch 100/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 957962584064.0000 - mae: 722259.6875 - val_loss: 1840124395520.0000 - val_mae: 999540.5000\n",
            "Epoch 101/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1072914956288.0000 - mae: 744726.3750 - val_loss: 1827813851136.0000 - val_mae: 1000433.7500\n",
            "Epoch 102/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1075265863680.0000 - mae: 744479.9375 - val_loss: 1835664146432.0000 - val_mae: 998427.0625\n",
            "Epoch 103/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 942014267392.0000 - mae: 707040.8125 - val_loss: 1844510064640.0000 - val_mae: 995961.3125\n",
            "Epoch 104/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 972579209216.0000 - mae: 731332.1875 - val_loss: 1834825547776.0000 - val_mae: 997307.3125\n",
            "Epoch 105/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1098334470144.0000 - mae: 742125.3750 - val_loss: 1821039656960.0000 - val_mae: 999564.0625\n",
            "Epoch 106/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1088870088704.0000 - mae: 749552.9375 - val_loss: 1826499854336.0000 - val_mae: 997069.8125\n",
            "Epoch 107/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 981835579392.0000 - mae: 724649.6875 - val_loss: 1830602670080.0000 - val_mae: 995669.0000\n",
            "Epoch 108/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 904191016960.0000 - mae: 700197.4375 - val_loss: 1836142297088.0000 - val_mae: 994373.0625\n",
            "Epoch 109/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1096618016768.0000 - mae: 774250.8125 - val_loss: 1811103088640.0000 - val_mae: 998364.1875\n",
            "Epoch 110/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 965461606400.0000 - mae: 707265.6250 - val_loss: 1814801022976.0000 - val_mae: 996768.6875\n",
            "Epoch 111/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 944571154432.0000 - mae: 710005.5625 - val_loss: 1821223550976.0000 - val_mae: 994665.3750\n",
            "Epoch 112/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1111636967424.0000 - mae: 749702.1250 - val_loss: 1812381564928.0000 - val_mae: 995464.6875\n",
            "Epoch 113/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1054378557440.0000 - mae: 736338.8750 - val_loss: 1820981460992.0000 - val_mae: 993439.1250\n",
            "Epoch 114/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 988645818368.0000 - mae: 729448.3125 - val_loss: 1824912834560.0000 - val_mae: 991923.8750\n",
            "Epoch 115/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 964342120448.0000 - mae: 726329.2500 - val_loss: 1816346361856.0000 - val_mae: 993319.6250\n",
            "Epoch 116/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 971286773760.0000 - mae: 720278.1250 - val_loss: 1804760121344.0000 - val_mae: 995247.6250\n",
            "Epoch 117/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1011123027968.0000 - mae: 722488.9375 - val_loss: 1809577017344.0000 - val_mae: 993911.4375\n",
            "Epoch 118/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1148543565824.0000 - mae: 762006.1875 - val_loss: 1808842358784.0000 - val_mae: 993225.1250\n",
            "Epoch 119/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 960531202048.0000 - mae: 728982.9375 - val_loss: 1821893197824.0000 - val_mae: 990584.8125\n",
            "Epoch 120/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 908401573888.0000 - mae: 695582.3750 - val_loss: 1796172283904.0000 - val_mae: 994384.1250\n",
            "Epoch 121/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1139972505600.0000 - mae: 769459.1250 - val_loss: 1801600237568.0000 - val_mae: 992205.6250\n",
            "Epoch 122/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1097490890752.0000 - mae: 738254.8125 - val_loss: 1797677252608.0000 - val_mae: 992576.3125\n",
            "Epoch 123/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 881249681408.0000 - mae: 693535.9375 - val_loss: 1807592062976.0000 - val_mae: 990265.8125\n",
            "Epoch 124/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 904419082240.0000 - mae: 685152.8125 - val_loss: 1803166023680.0000 - val_mae: 990177.8125\n",
            "Epoch 125/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 864776093696.0000 - mae: 663167.7500 - val_loss: 1796807983104.0000 - val_mae: 991623.5000\n",
            "Epoch 126/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1013944221696.0000 - mae: 728787.6250 - val_loss: 1791391825920.0000 - val_mae: 991689.8125\n",
            "Epoch 127/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 970342465536.0000 - mae: 717530.3750 - val_loss: 1799405043712.0000 - val_mae: 989812.3125\n",
            "Epoch 128/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 952880267264.0000 - mae: 690330.1875 - val_loss: 1792095420416.0000 - val_mae: 990713.7500\n",
            "Epoch 129/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 974434992128.0000 - mae: 716177.8750 - val_loss: 1798144655360.0000 - val_mae: 989104.9375\n",
            "Epoch 130/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1093638553600.0000 - mae: 733180.1250 - val_loss: 1799682129920.0000 - val_mae: 988762.8750\n",
            "Epoch 131/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 954394148864.0000 - mae: 716649.0625 - val_loss: 1799495352320.0000 - val_mae: 988604.1250\n",
            "Epoch 132/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 954655703040.0000 - mae: 716749.3125 - val_loss: 1784200560640.0000 - val_mae: 991114.5625\n",
            "Epoch 133/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 954098712576.0000 - mae: 718899.2500 - val_loss: 1792927203328.0000 - val_mae: 988376.0000\n",
            "Epoch 134/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 913734696960.0000 - mae: 691993.1875 - val_loss: 1793690959872.0000 - val_mae: 987752.5625\n",
            "Epoch 135/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 940731727872.0000 - mae: 715816.6250 - val_loss: 1791944425472.0000 - val_mae: 988214.6875\n",
            "Epoch 136/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1037892321280.0000 - mae: 751004.8750 - val_loss: 1797567545344.0000 - val_mae: 986583.5625\n",
            "Epoch 137/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1043904266240.0000 - mae: 734698.7500 - val_loss: 1788106113024.0000 - val_mae: 987829.4375\n",
            "Epoch 138/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 981319483392.0000 - mae: 708276.1250 - val_loss: 1789505306624.0000 - val_mae: 987199.8750\n",
            "Epoch 139/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 968258224128.0000 - mae: 703097.3125 - val_loss: 1783363928064.0000 - val_mae: 988133.9375\n",
            "Epoch 140/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 966977323008.0000 - mae: 718189.5000 - val_loss: 1786533380096.0000 - val_mae: 986825.3750\n",
            "Epoch 141/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 899205365760.0000 - mae: 705850.3125 - val_loss: 1797391122432.0000 - val_mae: 985197.6250\n",
            "Epoch 142/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1025294467072.0000 - mae: 711508.9375 - val_loss: 1792462946304.0000 - val_mae: 986238.6875\n",
            "Epoch 143/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 896550961152.0000 - mae: 694018.3125 - val_loss: 1787927855104.0000 - val_mae: 986602.5625\n",
            "Epoch 144/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 892494741504.0000 - mae: 687057.3750 - val_loss: 1785474711552.0000 - val_mae: 986715.8750\n",
            "Epoch 145/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1074837258240.0000 - mae: 753193.8125 - val_loss: 1776876388352.0000 - val_mae: 988154.1875\n",
            "Epoch 146/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1014805233664.0000 - mae: 732123.3750 - val_loss: 1789476339712.0000 - val_mae: 985296.9375\n",
            "Epoch 147/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 924820504576.0000 - mae: 692622.0000 - val_loss: 1785705267200.0000 - val_mae: 985135.5000\n",
            "Epoch 148/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 995714269184.0000 - mae: 721077.3125 - val_loss: 1783519248384.0000 - val_mae: 985534.8125\n",
            "Epoch 149/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 905779412992.0000 - mae: 698306.6250 - val_loss: 1784163467264.0000 - val_mae: 985519.8750\n",
            "Epoch 150/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 971795922944.0000 - mae: 694385.3125 - val_loss: 1797437652992.0000 - val_mae: 982951.0000\n",
            "Training Mean Absolute Error: 719349.9375\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1729505001472.0000 - mae: 958586.0000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Mean Absolute Error: 982951.0\n",
            "Model saved as 'housing_price_model.h5'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Prediction Mean Absolute Error: 982950.875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.models import load_model\n",
        "\n",
        "# Step 1: Load and preprocess the dataset\n",
        "url = \"/content/Housing.csv\"\n",
        "dataset = pd.read_csv(url)\n",
        "\n",
        "# Assuming 'price' is the target column and the rest are features\n",
        "X = dataset.drop('price', axis=1)  # Features\n",
        "y = dataset['price']  # Target variable\n",
        "\n",
        "# ----> Identify and handle categorical columns (e.g., using OneHotEncoder) <----\n",
        "categorical_cols = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea', 'furnishingstatus']  # Replace with your categorical column names\n",
        "\n",
        "# Create a OneHotEncoder instance (we set handle_unknown='ignore' to avoid errors for unseen categories)\n",
        "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "\n",
        "# Fit and transform the categorical features\n",
        "encoded_cols = encoder.fit_transform(X[categorical_cols])\n",
        "\n",
        "# Create a DataFrame from the encoded features\n",
        "encoded_df = pd.DataFrame(encoded_cols, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "\n",
        "# Drop the original categorical columns and concatenate the encoded features\n",
        "X = X.drop(categorical_cols, axis=1)\n",
        "X = pd.concat([X, encoded_df], axis=1)\n",
        "\n",
        "# Step 2: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Build the ANN model\n",
        "model = Sequential()\n",
        "\n",
        "# Hidden layers based on the provided architecture\n",
        "model.add(Dense(15, input_dim=X_train.shape[1], activation='relu'))  # Hidden Layer 1\n",
        "model.add(Dense(20, activation='relu'))  # Hidden Layer 2\n",
        "model.add(Dense(25, activation='relu'))  # Hidden Layer 3\n",
        "model.add(Dense(20, activation='relu'))  # Hidden Layer 4\n",
        "model.add(Dense(15, activation='relu'))  # Hidden Layer 5\n",
        "\n",
        "# Output layer (since we're predicting a continuous value, we use linear activation)\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Step 5: Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "# Step 6: Train the model\n",
        "history = model.fit(X_train, y_train, epochs=150, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Step 7: Evaluate the model\n",
        "train_loss, train_mae = model.evaluate(X_train, y_train)\n",
        "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
        "\n",
        "print(f\"Training MAE: {train_mae}\")\n",
        "print(f\"Testing MAE: {test_mae}\")\n",
        "\n",
        "# Step 8: Save the model in .h5 format\n",
        "model.save('housing_price_predictor.h5')\n",
        "\n",
        "# Step 9: Load the model for deployment\n",
        "loaded_model = load_model('housing_price_predictor.h5')\n",
        "\n",
        "# Step 10: Make predictions with the loaded model\n",
        "predictions = loaded_model.predict(X_test)\n",
        "\n",
        "# Display predictions vs actual values for the first 10 examples\n",
        "for i in range(10):\n",
        "    print(f\"Predicted: {predictions[i][0]}, Actual: {y_test.iloc[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96v5qf-8n_h0",
        "outputId": "a3c48833-7940-4768-db46-98f272fd9658"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - loss: 24871622934528.0000 - mae: 4676493.5000 - val_loss: 30129992499200.0000 - val_mae: 5007536.5000\n",
            "Epoch 2/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 23793074438144.0000 - mae: 4595508.5000 - val_loss: 30129990402048.0000 - val_mae: 5007536.0000\n",
            "Epoch 3/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25298594693120.0000 - mae: 4698601.5000 - val_loss: 30129986207744.0000 - val_mae: 5007536.0000\n",
            "Epoch 4/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 25275307917312.0000 - mae: 4700920.5000 - val_loss: 30129971527680.0000 - val_mae: 5007534.5000\n",
            "Epoch 5/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25894810812416.0000 - mae: 4775050.0000 - val_loss: 30129935876096.0000 - val_mae: 5007531.0000\n",
            "Epoch 6/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25167017279488.0000 - mae: 4699390.0000 - val_loss: 30129856184320.0000 - val_mae: 5007523.0000\n",
            "Epoch 7/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 25318286950400.0000 - mae: 4710863.5000 - val_loss: 30129652760576.0000 - val_mae: 5007502.5000\n",
            "Epoch 8/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25571541123072.0000 - mae: 4711504.5000 - val_loss: 30129172512768.0000 - val_mae: 5007455.5000\n",
            "Epoch 9/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 24685364379648.0000 - mae: 4670704.0000 - val_loss: 30128096673792.0000 - val_mae: 5007350.0000\n",
            "Epoch 10/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 24928476725248.0000 - mae: 4662763.5000 - val_loss: 30125819166720.0000 - val_mae: 5007130.0000\n",
            "Epoch 11/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25596407054336.0000 - mae: 4691267.0000 - val_loss: 30121207529472.0000 - val_mae: 5006688.5000\n",
            "Epoch 12/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 25126284296192.0000 - mae: 4721960.5000 - val_loss: 30112397393920.0000 - val_mae: 5005855.5000\n",
            "Epoch 13/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 24739477192704.0000 - mae: 4674714.5000 - val_loss: 30096406609920.0000 - val_mae: 5004367.5000\n",
            "Epoch 14/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 24760553570304.0000 - mae: 4669542.0000 - val_loss: 30069282045952.0000 - val_mae: 5001865.5000\n",
            "Epoch 15/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 24114695766016.0000 - mae: 4616629.0000 - val_loss: 30024367341568.0000 - val_mae: 4997775.5000\n",
            "Epoch 16/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 23922143657984.0000 - mae: 4585906.0000 - val_loss: 29953536032768.0000 - val_mae: 4991370.0000\n",
            "Epoch 17/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 25201821614080.0000 - mae: 4704804.0000 - val_loss: 29845058748416.0000 - val_mae: 4981653.5000\n",
            "Epoch 18/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 25747894829056.0000 - mae: 4753280.5000 - val_loss: 29684133789696.0000 - val_mae: 4967319.0000\n",
            "Epoch 19/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 25701197545472.0000 - mae: 4749779.5000 - val_loss: 29454329970688.0000 - val_mae: 4946869.5000\n",
            "Epoch 20/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 25139787857920.0000 - mae: 4689675.0000 - val_loss: 29129384656896.0000 - val_mae: 4918014.0000\n",
            "Epoch 21/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 25910984048640.0000 - mae: 4723134.0000 - val_loss: 28686740881408.0000 - val_mae: 4878639.0000\n",
            "Epoch 22/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 24157332963328.0000 - mae: 4607865.0000 - val_loss: 28092651274240.0000 - val_mae: 4825515.0000\n",
            "Epoch 23/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 23151643721728.0000 - mae: 4505230.5000 - val_loss: 27314597396480.0000 - val_mae: 4755288.5000\n",
            "Epoch 24/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 22688548519936.0000 - mae: 4467482.5000 - val_loss: 26323525304320.0000 - val_mae: 4664420.5000\n",
            "Epoch 25/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 21314012184576.0000 - mae: 4321064.0000 - val_loss: 25076386758656.0000 - val_mae: 4547758.0000\n",
            "Epoch 26/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 20349615865856.0000 - mae: 4205854.0000 - val_loss: 23528659222528.0000 - val_mae: 4399006.5000\n",
            "Epoch 27/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 18215109394432.0000 - mae: 4005221.2500 - val_loss: 21748705656832.0000 - val_mae: 4220275.5000\n",
            "Epoch 28/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 18148168302592.0000 - mae: 3947649.5000 - val_loss: 19608014684160.0000 - val_mae: 3995495.7500\n",
            "Epoch 29/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 15654605291520.0000 - mae: 3661811.7500 - val_loss: 17310288969728.0000 - val_mae: 3736047.0000\n",
            "Epoch 30/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12903971291136.0000 - mae: 3333009.7500 - val_loss: 14853832966144.0000 - val_mae: 3433987.7500\n",
            "Epoch 31/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11065433784320.0000 - mae: 3027821.5000 - val_loss: 12373602598912.0000 - val_mae: 3093790.2500\n",
            "Epoch 32/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9081991462912.0000 - mae: 2707557.2500 - val_loss: 9982349148160.0000 - val_mae: 2718617.5000\n",
            "Epoch 33/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6679280222208.0000 - mae: 2242907.2500 - val_loss: 7898262405120.0000 - val_mae: 2335443.5000\n",
            "Epoch 34/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5510051397632.0000 - mae: 1964243.0000 - val_loss: 6236609708032.0000 - val_mae: 2006933.0000\n",
            "Epoch 35/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3883967447040.0000 - mae: 1584979.3750 - val_loss: 5024149667840.0000 - val_mae: 1745754.2500\n",
            "Epoch 36/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3811851894784.0000 - mae: 1514617.8750 - val_loss: 4258142617600.0000 - val_mae: 1603207.7500\n",
            "Epoch 37/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2916109254656.0000 - mae: 1304510.7500 - val_loss: 3829888974848.0000 - val_mae: 1527627.2500\n",
            "Epoch 38/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2447777464320.0000 - mae: 1177583.2500 - val_loss: 3553376075776.0000 - val_mae: 1480515.3750\n",
            "Epoch 39/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2453731278848.0000 - mae: 1130811.6250 - val_loss: 3398793953280.0000 - val_mae: 1450138.7500\n",
            "Epoch 40/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2142941085696.0000 - mae: 1103431.6250 - val_loss: 3299446095872.0000 - val_mae: 1429900.0000\n",
            "Epoch 41/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2239741034496.0000 - mae: 1115695.8750 - val_loss: 3226459439104.0000 - val_mae: 1417287.5000\n",
            "Epoch 42/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2236436447232.0000 - mae: 1104184.2500 - val_loss: 3173741232128.0000 - val_mae: 1407316.7500\n",
            "Epoch 43/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2044338241536.0000 - mae: 1060817.1250 - val_loss: 3101357244416.0000 - val_mae: 1392428.3750\n",
            "Epoch 44/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2194282774528.0000 - mae: 1089997.3750 - val_loss: 3065255297024.0000 - val_mae: 1384812.5000\n",
            "Epoch 45/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1996065865728.0000 - mae: 1036137.0625 - val_loss: 3020794888192.0000 - val_mae: 1374282.6250\n",
            "Epoch 46/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1920603389952.0000 - mae: 1018063.1250 - val_loss: 2978552741888.0000 - val_mae: 1364687.2500\n",
            "Epoch 47/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1780516913152.0000 - mae: 968003.3750 - val_loss: 2939762769920.0000 - val_mae: 1354469.6250\n",
            "Epoch 48/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1986137161728.0000 - mae: 1018819.5000 - val_loss: 2916817305600.0000 - val_mae: 1347509.0000\n",
            "Epoch 49/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1869230505984.0000 - mae: 1026483.9375 - val_loss: 2877571465216.0000 - val_mae: 1337263.5000\n",
            "Epoch 50/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1812099497984.0000 - mae: 983740.5625 - val_loss: 2833112629248.0000 - val_mae: 1326503.6250\n",
            "Epoch 51/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1576130445312.0000 - mae: 977173.3750 - val_loss: 2819602251776.0000 - val_mae: 1321028.3750\n",
            "Epoch 52/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1557636841472.0000 - mae: 921909.1250 - val_loss: 2800296394752.0000 - val_mae: 1315782.0000\n",
            "Epoch 53/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1789645553664.0000 - mae: 991992.0000 - val_loss: 2765600325632.0000 - val_mae: 1307200.5000\n",
            "Epoch 54/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1830545129472.0000 - mae: 983009.1875 - val_loss: 2743056465920.0000 - val_mae: 1301914.6250\n",
            "Epoch 55/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1415671185408.0000 - mae: 881390.1250 - val_loss: 2731033493504.0000 - val_mae: 1298630.8750\n",
            "Epoch 56/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1633830699008.0000 - mae: 943807.3750 - val_loss: 2697077456896.0000 - val_mae: 1292105.0000\n",
            "Epoch 57/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1639995932672.0000 - mae: 939915.3750 - val_loss: 2679360978944.0000 - val_mae: 1287310.5000\n",
            "Epoch 58/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1751051796480.0000 - mae: 1002068.2500 - val_loss: 2654788386816.0000 - val_mae: 1283400.2500\n",
            "Epoch 59/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1416006336512.0000 - mae: 895087.5000 - val_loss: 2644435795968.0000 - val_mae: 1280174.3750\n",
            "Epoch 60/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1456090644480.0000 - mae: 931252.0000 - val_loss: 2640253812736.0000 - val_mae: 1276627.8750\n",
            "Epoch 61/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1544633057280.0000 - mae: 925220.4375 - val_loss: 2607903932416.0000 - val_mae: 1271176.5000\n",
            "Epoch 62/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1632390742016.0000 - mae: 966153.3125 - val_loss: 2585900089344.0000 - val_mae: 1266235.6250\n",
            "Epoch 63/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1591284072448.0000 - mae: 948774.5625 - val_loss: 2572564299776.0000 - val_mae: 1263171.8750\n",
            "Epoch 64/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1471780618240.0000 - mae: 934467.5000 - val_loss: 2558385192960.0000 - val_mae: 1258287.8750\n",
            "Epoch 65/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1346861400064.0000 - mae: 870411.2500 - val_loss: 2547100418048.0000 - val_mae: 1254927.3750\n",
            "Epoch 66/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1439063343104.0000 - mae: 893893.1250 - val_loss: 2528125386752.0000 - val_mae: 1249830.2500\n",
            "Epoch 67/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1403659747328.0000 - mae: 887625.3125 - val_loss: 2508505219072.0000 - val_mae: 1245468.8750\n",
            "Epoch 68/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1227551277056.0000 - mae: 825143.5625 - val_loss: 2500073619456.0000 - val_mae: 1242735.8750\n",
            "Epoch 69/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1461579153408.0000 - mae: 891194.8125 - val_loss: 2490006765568.0000 - val_mae: 1239507.8750\n",
            "Epoch 70/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1188893949952.0000 - mae: 810583.3750 - val_loss: 2457975914496.0000 - val_mae: 1233854.0000\n",
            "Epoch 71/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1362978668544.0000 - mae: 887071.3125 - val_loss: 2460682289152.0000 - val_mae: 1231084.6250\n",
            "Epoch 72/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1216631668736.0000 - mae: 829754.7500 - val_loss: 2449429233664.0000 - val_mae: 1228286.3750\n",
            "Epoch 73/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1294169407488.0000 - mae: 862142.0000 - val_loss: 2446550368256.0000 - val_mae: 1225529.5000\n",
            "Epoch 74/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1230885617664.0000 - mae: 829136.4375 - val_loss: 2432196149248.0000 - val_mae: 1221849.8750\n",
            "Epoch 75/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1254635470848.0000 - mae: 839614.6875 - val_loss: 2424490688512.0000 - val_mae: 1218978.6250\n",
            "Epoch 76/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1165666942976.0000 - mae: 813861.0000 - val_loss: 2403770040320.0000 - val_mae: 1215033.8750\n",
            "Epoch 77/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1173354708992.0000 - mae: 829089.6250 - val_loss: 2406246776832.0000 - val_mae: 1212822.5000\n",
            "Epoch 78/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1285014945792.0000 - mae: 855097.0625 - val_loss: 2382091517952.0000 - val_mae: 1208083.3750\n",
            "Epoch 79/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1141617852416.0000 - mae: 817118.5000 - val_loss: 2385581703168.0000 - val_mae: 1205667.1250\n",
            "Epoch 80/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1205608382464.0000 - mae: 827901.1250 - val_loss: 2363522547712.0000 - val_mae: 1201464.5000\n",
            "Epoch 81/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1054051008512.0000 - mae: 785704.9375 - val_loss: 2357780021248.0000 - val_mae: 1198615.2500\n",
            "Epoch 82/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1176381685760.0000 - mae: 840390.3750 - val_loss: 2352817111040.0000 - val_mae: 1195643.6250\n",
            "Epoch 83/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1133139984384.0000 - mae: 802417.1250 - val_loss: 2333319626752.0000 - val_mae: 1191708.7500\n",
            "Epoch 84/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1250647474176.0000 - mae: 840682.1250 - val_loss: 2338372714496.0000 - val_mae: 1189705.3750\n",
            "Epoch 85/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1214892081152.0000 - mae: 839742.6250 - val_loss: 2334739136512.0000 - val_mae: 1187258.3750\n",
            "Epoch 86/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1235490832384.0000 - mae: 852698.6875 - val_loss: 2328481497088.0000 - val_mae: 1184576.8750\n",
            "Epoch 87/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1156238671872.0000 - mae: 826057.6875 - val_loss: 2318621999104.0000 - val_mae: 1181233.3750\n",
            "Epoch 88/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1149206003712.0000 - mae: 808883.5625 - val_loss: 2306705981440.0000 - val_mae: 1177570.0000\n",
            "Epoch 89/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1133100924928.0000 - mae: 816972.1875 - val_loss: 2313004515328.0000 - val_mae: 1175633.6250\n",
            "Epoch 90/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1022038704128.0000 - mae: 783431.6250 - val_loss: 2292307722240.0000 - val_mae: 1171455.8750\n",
            "Epoch 91/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1154901213184.0000 - mae: 821358.3125 - val_loss: 2276253761536.0000 - val_mae: 1168539.7500\n",
            "Epoch 92/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1096678047744.0000 - mae: 791187.6875 - val_loss: 2284738576384.0000 - val_mae: 1166899.8750\n",
            "Epoch 93/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1147140702208.0000 - mae: 829534.9375 - val_loss: 2292339179520.0000 - val_mae: 1166065.6250\n",
            "Epoch 94/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1098512007168.0000 - mae: 794068.5625 - val_loss: 2273328234496.0000 - val_mae: 1161743.0000\n",
            "Epoch 95/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1079749509120.0000 - mae: 785167.4375 - val_loss: 2271208800256.0000 - val_mae: 1159998.8750\n",
            "Epoch 96/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1038965080064.0000 - mae: 787457.8750 - val_loss: 2256811065344.0000 - val_mae: 1156234.6250\n",
            "Epoch 97/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1132154060800.0000 - mae: 796373.0625 - val_loss: 2251922079744.0000 - val_mae: 1153933.3750\n",
            "Epoch 98/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1216565608448.0000 - mae: 821719.0000 - val_loss: 2247757135872.0000 - val_mae: 1151885.7500\n",
            "Epoch 99/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1158971785216.0000 - mae: 822652.4375 - val_loss: 2243016261632.0000 - val_mae: 1149758.2500\n",
            "Epoch 100/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1047884005376.0000 - mae: 783291.6875 - val_loss: 2242210168832.0000 - val_mae: 1147989.2500\n",
            "Epoch 101/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1105274994688.0000 - mae: 798183.8125 - val_loss: 2232863948800.0000 - val_mae: 1145462.3750\n",
            "Epoch 102/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1054296899584.0000 - mae: 779691.1250 - val_loss: 2224219750400.0000 - val_mae: 1143070.5000\n",
            "Epoch 103/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1189263310848.0000 - mae: 841088.5000 - val_loss: 2223058976768.0000 - val_mae: 1141684.8750\n",
            "Epoch 104/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1044323893248.0000 - mae: 772871.7500 - val_loss: 2217836281856.0000 - val_mae: 1139346.3750\n",
            "Epoch 105/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 921703284736.0000 - mae: 740579.9375 - val_loss: 2211709190144.0000 - val_mae: 1137027.2500\n",
            "Epoch 106/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1120811876352.0000 - mae: 809283.9375 - val_loss: 2190595850240.0000 - val_mae: 1132776.6250\n",
            "Epoch 107/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 975015378944.0000 - mae: 756108.9375 - val_loss: 2193503682560.0000 - val_mae: 1131536.5000\n",
            "Epoch 108/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1137722523648.0000 - mae: 809253.3750 - val_loss: 2183939358720.0000 - val_mae: 1129446.6250\n",
            "Epoch 109/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 930446311424.0000 - mae: 721457.0625 - val_loss: 2189697613824.0000 - val_mae: 1128728.0000\n",
            "Epoch 110/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1180332195840.0000 - mae: 816087.6250 - val_loss: 2189947961344.0000 - val_mae: 1127459.2500\n",
            "Epoch 111/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1257448013824.0000 - mae: 825949.0625 - val_loss: 2182383534080.0000 - val_mae: 1125059.5000\n",
            "Epoch 112/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1093726371840.0000 - mae: 779995.6875 - val_loss: 2172264251392.0000 - val_mae: 1122873.1250\n",
            "Epoch 113/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1136663461888.0000 - mae: 793607.0625 - val_loss: 2162125963264.0000 - val_mae: 1120675.0000\n",
            "Epoch 114/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1035772690432.0000 - mae: 762964.9375 - val_loss: 2172789194752.0000 - val_mae: 1120219.5000\n",
            "Epoch 115/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1020968632320.0000 - mae: 750952.0625 - val_loss: 2160274964480.0000 - val_mae: 1117841.3750\n",
            "Epoch 116/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 978581323776.0000 - mae: 751615.5625 - val_loss: 2166876405760.0000 - val_mae: 1117346.8750\n",
            "Epoch 117/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 936780562432.0000 - mae: 749016.5000 - val_loss: 2158717960192.0000 - val_mae: 1115264.7500\n",
            "Epoch 118/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1099880333312.0000 - mae: 790290.8750 - val_loss: 2149820923904.0000 - val_mae: 1113396.6250\n",
            "Epoch 119/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1046404268032.0000 - mae: 779979.3125 - val_loss: 2144140656640.0000 - val_mae: 1111645.1250\n",
            "Epoch 120/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1054192304128.0000 - mae: 772922.0000 - val_loss: 2141959880704.0000 - val_mae: 1110086.8750\n",
            "Epoch 121/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 967366344704.0000 - mae: 763265.0000 - val_loss: 2154737041408.0000 - val_mae: 1110440.6250\n",
            "Epoch 122/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 905092726784.0000 - mae: 724623.2500 - val_loss: 2149641093120.0000 - val_mae: 1109020.3750\n",
            "Epoch 123/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 876626378752.0000 - mae: 724969.0000 - val_loss: 2137763479552.0000 - val_mae: 1106748.7500\n",
            "Epoch 124/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1085765779456.0000 - mae: 776597.7500 - val_loss: 2118758301696.0000 - val_mae: 1103294.8750\n",
            "Epoch 125/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 955857174528.0000 - mae: 748825.4375 - val_loss: 2130400378880.0000 - val_mae: 1103939.6250\n",
            "Epoch 126/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 949793849344.0000 - mae: 740471.4375 - val_loss: 2131191529472.0000 - val_mae: 1103045.2500\n",
            "Epoch 127/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 941222395904.0000 - mae: 750405.2500 - val_loss: 2129165811712.0000 - val_mae: 1101991.1250\n",
            "Epoch 128/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 874535649280.0000 - mae: 706353.4375 - val_loss: 2128883744768.0000 - val_mae: 1101400.2500\n",
            "Epoch 129/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 986717421568.0000 - mae: 752141.5000 - val_loss: 2111054938112.0000 - val_mae: 1098151.6250\n",
            "Epoch 130/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1020120399872.0000 - mae: 774026.0625 - val_loss: 2111672418304.0000 - val_mae: 1097949.6250\n",
            "Epoch 131/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 943337177088.0000 - mae: 735330.1875 - val_loss: 2120151990272.0000 - val_mae: 1098001.8750\n",
            "Epoch 132/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1050651852800.0000 - mae: 786609.2500 - val_loss: 2117128945664.0000 - val_mae: 1096469.3750\n",
            "Epoch 133/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1075213697024.0000 - mae: 792531.9375 - val_loss: 2102174679040.0000 - val_mae: 1094561.7500\n",
            "Epoch 134/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 892341387264.0000 - mae: 729467.6875 - val_loss: 2117440372736.0000 - val_mae: 1095189.7500\n",
            "Epoch 135/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 918160408576.0000 - mae: 713705.3125 - val_loss: 2101545795584.0000 - val_mae: 1092160.0000\n",
            "Epoch 136/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1038136836096.0000 - mae: 771045.4375 - val_loss: 2099293847552.0000 - val_mae: 1091413.1250\n",
            "Epoch 137/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 995889774592.0000 - mae: 736407.6875 - val_loss: 2090567467008.0000 - val_mae: 1089745.7500\n",
            "Epoch 138/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 947614711808.0000 - mae: 711655.2500 - val_loss: 2097980112896.0000 - val_mae: 1089732.8750\n",
            "Epoch 139/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1187182673920.0000 - mae: 821299.3125 - val_loss: 2096046145536.0000 - val_mae: 1089607.8750\n",
            "Epoch 140/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1173345009664.0000 - mae: 809272.4375 - val_loss: 2092162220032.0000 - val_mae: 1088360.2500\n",
            "Epoch 141/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 967604043776.0000 - mae: 754801.3125 - val_loss: 2103182884864.0000 - val_mae: 1088597.7500\n",
            "Epoch 142/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 954585317376.0000 - mae: 749075.6250 - val_loss: 2089131966464.0000 - val_mae: 1086654.5000\n",
            "Epoch 143/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 927609585664.0000 - mae: 733921.3750 - val_loss: 2074604077056.0000 - val_mae: 1084444.6250\n",
            "Epoch 144/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 856366317568.0000 - mae: 713419.6250 - val_loss: 2096975970304.0000 - val_mae: 1085652.3750\n",
            "Epoch 145/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 934631440384.0000 - mae: 736503.3125 - val_loss: 2080894746624.0000 - val_mae: 1083690.2500\n",
            "Epoch 146/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 960786989056.0000 - mae: 739408.4375 - val_loss: 2063430451200.0000 - val_mae: 1081231.2500\n",
            "Epoch 147/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 962951839744.0000 - mae: 747800.1250 - val_loss: 2094697021440.0000 - val_mae: 1083785.7500\n",
            "Epoch 148/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 910191362048.0000 - mae: 721130.3125 - val_loss: 2078136205312.0000 - val_mae: 1081171.7500\n",
            "Epoch 149/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 896438370304.0000 - mae: 717886.4375 - val_loss: 2065661165568.0000 - val_mae: 1079069.2500\n",
            "Epoch 150/150\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1058282864640.0000 - mae: 774353.7500 - val_loss: 2065186029568.0000 - val_mae: 1078217.7500\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 958016061440.0000 - mae: 728576.1250 \n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1934557708288.0000 - mae: 1044114.8750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training MAE: 730196.25\n",
            "Testing MAE: 1078217.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7ed0528460c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 71ms/step"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7ed0528460c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Predicted: 3907546.5, Actual: 4060000\n",
            "Predicted: 7772574.5, Actual: 6650000\n",
            "Predicted: 3577275.0, Actual: 3710000\n",
            "Predicted: 4678208.5, Actual: 6440000\n",
            "Predicted: 3616733.25, Actual: 2800000\n",
            "Predicted: 3502140.75, Actual: 4900000\n",
            "Predicted: 5085620.0, Actual: 5250000\n",
            "Predicted: 6134715.5, Actual: 4543000\n",
            "Predicted: 1544033.125, Actual: 2450000\n",
            "Predicted: 2868396.0, Actual: 3353000\n"
          ]
        }
      ]
    }
  ]
}